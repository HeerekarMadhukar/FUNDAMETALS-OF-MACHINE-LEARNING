---
title: "Machine Learning Assignment 2"
author: "Madhukar Heerekar"
date: '2022-03-17'
output: pdf_document
---


```{r}
UB <- read.csv("c://users//heere//OneDrive//Desktop//FML ASSIGNMENT 2//universalbank .csv")
summary(UB)
```

```{r}
library(caret)
UB$Personal.Loan<- as.factor(UB$Personal.Loan)
library(dummies)
dummy_model<-dummyVars(~Education, data=UB)
head(predict(dummy_model,UB))
UB_dummy <-dummy.data.frame(UB, names =c("Education"), sep="-")
UBank<-subset(UB_dummy,select = -c(1, 5))
head(UBank)
```

```{r}
##Splitting the Data into Test and validation
set.seed(15)
Train_Index <-createDataPartition(UBank$Personal.Loan,p=0.6, list = FALSE)
```

```{r}
#use 60% for training and the remaining for validation
Train <-UBank[Train_Index,]
Valid <- UBank[-Train_Index,]
train.norm.df<-Train
valid.norm.df<-Valid
```

```{r}
##Normalising the Data
norm.values<-preProcess(Train[,-10], method = c("range"))
train.norm.df[,-10] <- predict(norm.values,Train[,-10])
valid.norm.df[,-10]<-predict(norm.values,Valid[,-10])
```

```{r}
##Modelling using K=1
library(FNN)
nn <- knn(train = train.norm.df[, -10], test = valid.norm.df[, -10], 
          cl = train.norm.df[, 10], k = 1, prob=TRUE)
```

```{r}
head(nn)
##value of k that provides the best performance
library(caret)
accuracy.df <-data.frame(k= seq(1,14,1), accuracy = rep(0,14))
for(i in 1:14) {
                  knn <- knn(train.norm.df[, -10], valid.norm.df[, -10], cl = train.norm.df[, 10], k = i)
                  accuracy.df[i, 2] <- confusionMatrix(knn, valid.norm.df[, 10])$overall[1] 
                }
accuracy.df
which.max((accuracy.df$accuracy))
```

```{r}
##Test data development
L_Predictors<-UBank[,-10]
L_labels<-UBank[,10]
Test <- data.frame(40, 10, 84, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1)
colnames(Test) <- colnames(L_Predictors)
Test.norm.df <- Test
head(Test.norm.df)
```

```{r}
##combining Training and Validation set to normalise new set
Traval.norm.df <- UBank
norm.values <- preProcess(UBank[,-10], method = c("range"))
Traval.norm.df[,-10]<-predict(norm.values, UBank[,-10])
Test.norm.df<-predict(norm.values, Test)
```

```{r}
##Predicting using k=1
nn <- knn(train = Traval.norm.df[, -10], test = Test.norm.df, 
          cl = Traval.norm.df[, 10], k = 1, prob=TRUE)
```

```{r}
##View predicted class
head(nn)
```

## If a Customer is classified as zero, customer will not accept the loan

```{r}
##Predicting using k=3
nn <- knn(train = Traval.norm.df[, -10], test = Test.norm.df, 
cl = Traval.norm.df[, 10], k = 3, prob=TRUE)
```

```{r}
##View predicted class
head(nn)
##Customer classified as zero, customer will not accept the loan
```

```{r}
##Show the confusion matrix for the validation data that results from using the best k.
knn.valid <- knn(train.norm.df[, -10],valid.norm.df[, -10],cl=train.norm.df[, 10],k=3,prob = 0.5)
confusionMatrix(knn.valid, valid.norm.df[, 10])
```

```{r}
##Error types 
##True Negative - 1794
##False Negative - 14
##True Positive - 118
##False Positive - 74
##Sensitivity(TPR) - TP/(TP+FN) = 118/(118+14)=0.8939
#specificity(TNR)- TN/(TN+FP) = 1794/(1794+74)=0.9603
```

```{r}
#modelling with diff partitioning -  training, validation, and test sets (50% : 30% : 20%)
#split the data 
set.seed(15)
Train_Index_2 <-createDataPartition(UBank$Personal.Loan,p=0.5, list = FALSE)
```

```{r}
#use 50% for training and the rest for validation and test
Train_2 <-UBank[Train_Index_2,]
ValTest <- UBank[-Train_Index_2,]
Valid_Index  <- createDataPartition(ValTest$Personal.Loan,p=0.6, list = FALSE)
Valid_2 <- ValTest[Valid_Index,]
Test_2 <- ValTest[-Valid_Index,]
```

```{r}
#copy original data
train_2.norm.df<-Train_2
valid_2.norm.df<-Valid_2
test_2.norm.df <-Test_2
```

```{r}
#normalize data
norm.values_2<-preProcess(Train_2[,-10], method = c("center", "scale"))
train_2.norm.df[,-10] <- predict(norm.values_2,Train_2[,-10])
valid_2.norm.df[,-10]<-predict(norm.values_2,Valid_2[,-10])
test_2.norm.df[,-10]<-predict(norm.values_2,Test_2[,-10])
```

```{r}
#Modelling using k=3 for testset
library(FNN)
nn_2 <- knn(train = train_2.norm.df[, -10], test = test_2.norm.df[, -10], 
          cl = train_2.norm.df[, 10], k = 3, prob=TRUE)
```

```{r}
#view predicted class
head(nn_2)
```

```{r}
#Modelling using k=3 for validation set
nn_2_valid<- knn(train = train_2.norm.df[, -10], test = valid_2.norm.df[, -10], 
          cl = train_2.norm.df[, 10], k = 3, prob=TRUE)
```

```{r}
#view predicted class
head(nn_2_valid)
```

```{r}
#compare confusion matrix for test set with validation set
confusionMatrix(nn_2, test_2.norm.df[, 10])
```

```{r}
#Accuracy for Test is 0.956
confusionMatrix(nn_2_valid, valid_2.norm.df[, 10])
```

#Accuracy for validation is 0.954 & test set is 0.956.From the above, comparing confusion matrix of the test set with that of the training and validation sets we can determine that a slightly higher training set means that there is no over fitting of data and found the #better value of k.